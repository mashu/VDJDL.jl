var documenterSearchIndex = {"docs":
[{"location":"embeddings/","page":"Embeddings","title":"Embeddings","text":"CurrentModule = VDJDL.Embeddings","category":"page"},{"location":"embeddings/","page":"Embeddings","title":"Embeddings","text":"Modules = [Embeddings]\nOrder   = [:function, :type]","category":"page"},{"location":"embeddings/#Base.show-Tuple{IO, VDJDL.Embeddings.PositionEncoding}","page":"Embeddings","title":"Base.show","text":"Base.show(io::IO, pe::PositionEncoding)\n\nDisplays the PositionEncoding object.\n\nArguments\n\nio::IO: The I/O stream.\npe::PositionEncoding: The PositionEncoding object.\n\n\n\n\n\n","category":"method"},{"location":"embeddings/#VDJDL.Embeddings.make_position_encoding","page":"Embeddings","title":"VDJDL.Embeddings.make_position_encoding","text":"make_position_encoding(dim_embedding::Int, seq_length::Int, n::Int=10000)\n\nGenerates a positional encoding matrix.\n\nArguments\n\ndim_embedding::Int: The dimension of the embedding.\nseq_length::Int: The length of the sequence.\nn::Int: A scaling factor. Defaults to 10000.\n\nReturns\n\nMatrix{Float32}: A matrix containing positional encodings.\n\n\n\n\n\n","category":"function"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"PositionEncoding(dim_embedding::Int, max_length::Int=1000)\n\nConstructs a PositionEncoding object with specified embedding dimension and maximum sequence length.\n\nArguments\n\ndim_embedding::Int: The dimension of the embedding.\nmax_length::Int: The maximum length of the sequence. Defaults to 1000.\n\nReturns\n\nPositionEncoding: A PositionEncoding object.\n\n\n\n\n\n","category":"type"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-2","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"PositionEncoding{W <: AbstractArray}(weight::W)\n\nRepresents positional encodings for sequence data.\n\nArguments\n\nweight::W: The weight matrix for positional encodings.\n\n\n\n\n\n","category":"type"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-Tuple{AbstractArray}","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"(pe::PositionEncoding)(x::AbstractArray)\n\nApplies positional encoding to the input array.\n\nArguments\n\nx::AbstractArray: The input array.\n\nReturns\n\nThe positional encoding for the input array.\n\n\n\n\n\n","category":"method"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-Tuple{Int64}","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"(pe::PositionEncoding)(seq_length::Int)\n\nRetrieves the positional encoding for a specific sequence length.\n\nArguments\n\nseq_length::Int: The length of the sequence.\n\nReturns\n\nA view of the positional encoding matrix for the given sequence length.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/","page":"VDJDistributions","title":"VDJDistributions","text":"CurrentModule = VDJDL.VDJDistributions","category":"page"},{"location":"vdjdistributions/","page":"VDJDistributions","title":"VDJDistributions","text":"Modules = [VDJDistributions]\nOrder   = [:function, :type]","category":"page"},{"location":"vdjdistributions/#Base.rand-Tuple{Random.AbstractRNG, VDJDL.VDJDistributions.BiZISNB, Int64}","page":"VDJDistributions","title":"Base.rand","text":"Distributions.rand(rng::AbstractRNG, d::BiZISNB, n::Int)\n\nGenerates n random samples from the BiZISNB distribution.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Base.rand-Tuple{Random.AbstractRNG, VDJDL.VDJDistributions.BiZISNB}","page":"VDJDistributions","title":"Base.rand","text":"Distributions.rand(rng::AbstractRNG, d::BiZISNB)\n\nGenerates a random sample from the BiZISNB distribution.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Base.rand-Tuple{Random.AbstractRNG, VDJDL.VDJDistributions.MixtureZISNB}","page":"VDJDistributions","title":"Base.rand","text":"Distributions.rand(rng::AbstractRNG, d::MixtureZISNB)\n\nGenerates a random sample from the MixtureZISNB distribution.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Base.rand-Tuple{Random.AbstractRNG, VDJDL.VDJDistributions.ZISNB}","page":"VDJDistributions","title":"Base.rand","text":"Distributions.rand(rng::AbstractRNG, d::ZISNB)\n\nGenerates a random sample from the Zero-Inflated Shifted Negative Binomial distribution.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Base.rand-Tuple{VDJDL.VDJDistributions.ShiftedPoisson}","page":"VDJDistributions","title":"Base.rand","text":"Base.rand(d::ShiftedPoisson)\n\nGenerates a random sample from the Shifted Poisson distribution.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Distributions.logpdf-Tuple{VDJDL.VDJDistributions.BiZISNB, Tuple{Int64, Int64}}","page":"VDJDistributions","title":"Distributions.logpdf","text":"Distributions.logpdf(d::BiZISNB, x::Tuple{Int, Int})\n\nCalculates the log of the probability density function for the BiZISNB distribution at a given tuple x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Distributions.logpdf-Tuple{VDJDL.VDJDistributions.MixtureZISNB, Real}","page":"VDJDistributions","title":"Distributions.logpdf","text":"Distributions.logpdf(d::MixtureZISNB, x::Real)\n\nCalculates the log of the probability density function for the MixtureZISNB distribution at a given value x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Distributions.logpdf-Tuple{VDJDL.VDJDistributions.ShiftedPoisson, Int64}","page":"VDJDistributions","title":"Distributions.logpdf","text":"Distributions.logpdf(d::ShiftedPoisson, x::Int)\n\nCalculates the log of the probability density function for the Shifted Poisson distribution at a given integer x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Distributions.logpdf-Tuple{VDJDL.VDJDistributions.ZISNB, Int64}","page":"VDJDistributions","title":"Distributions.logpdf","text":"Distributions.logpdf(d::ZISNB, x::Int)\n\nCalculates the log of the probability density function for the Zero-Inflated Shifted Negative Binomial distribution at a given integer x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#Distributions.pdf-Tuple{VDJDL.VDJDistributions.ShiftedPoisson, Int64}","page":"VDJDistributions","title":"Distributions.pdf","text":"Distributions.pdf(d::ShiftedPoisson, x::Int)\n\nCalculates the probability density function for the Shifted Poisson distribution at a given integer x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#StatsAPI.loglikelihood-Tuple{VDJDL.VDJDistributions.BiZISNB, Tuple{Int64, Int64}}","page":"VDJDistributions","title":"StatsAPI.loglikelihood","text":"Distributions.loglikelihood(d::BiZISNB, x::Tuple{Int, Int})\n\nCalculates the log likelihood for the BiZISNB distribution at a given tuple x.\n\n\n\n\n\n","category":"method"},{"location":"vdjdistributions/#VDJDL.VDJDistributions.BiZISNB","page":"VDJDistributions","title":"VDJDL.VDJDistributions.BiZISNB","text":"BiZISNB{T1, T2, L}(π_left, p_left, π_right, p_right, α, β, len)\n\nDefines the BiZISNB distribution that combines two ZISNB distributions.\n\nπ_left: Probability of zero inflation for the left trimming.\np_left: Probability of success for the left trimming.\nπ_right: Probability of zero inflation for the right trimming.\np_right: Probability of success for the right trimming.\nα: Coefficient for the exponential part of the distribution.\nβ: Coefficient for the length-dependent part of the distribution.\nlen: Length parameter.\n\n\n\n\n\n","category":"type"},{"location":"vdjdistributions/#VDJDL.VDJDistributions.MixtureZISNB","page":"VDJDistributions","title":"VDJDL.VDJDistributions.MixtureZISNB","text":"MixtureZISNB{T}(zisnb, negative_min, negative_max, mixture_prob)\n\nDefines the MixtureZISNB distribution that combines the ZISNB and DiscreteUniform distributions for negative values.\n\nzisnb: The Zero-Inflated Shifted Negative Binomial component.\nnegative_min: Minimum value for the negative range.\nnegative_max: Maximum value for the negative range.\nmixture_prob: Probability of selecting the ZISNB component.\n\n\n\n\n\n","category":"type"},{"location":"vdjdistributions/#VDJDL.VDJDistributions.ShiftedPoisson","page":"VDJDistributions","title":"VDJDL.VDJDistributions.ShiftedPoisson","text":"ShiftedPoisson{T}(λ, shift)\n\nDefines the Shifted Poisson distribution.\n\nλ: The rate parameter of the Poisson distribution.\nshift: The integer value by which the Poisson distribution is shifted.\n\n\n\n\n\n","category":"type"},{"location":"vdjdistributions/#VDJDL.VDJDistributions.ZISNB","page":"VDJDistributions","title":"VDJDL.VDJDistributions.ZISNB","text":"ZISNB{T}(π, r, p)\n\nDefines the Zero-Inflated Shifted Negative Binomial distribution.\n\nπ: Probability of zero inflation.\nr: Number of successes for the negative binomial component.\np: Probability of success for the negative binomial component.\n\n\n\n\n\n","category":"type"},{"location":"layers/","page":"Layers","title":"Layers","text":"CurrentModule = VDJDL.Layers","category":"page"},{"location":"layers/","page":"Layers","title":"Layers","text":"Modules = [Layers]\nOrder   = [:function, :type]","category":"page"},{"location":"layers/#Base.show-Tuple{IO, VDJDL.Layers.Rezero}","page":"Layers","title":"Base.show","text":"Show the Rezero layer in a human-readable format.\n\nArguments\n\nio::IO: The I/O stream.\nm::Rezero: The Rezero layer instance.\n\n\n\n\n\n","category":"method"},{"location":"layers/#Base.show-Tuple{IO, VDJDL.Layers.RotaryMultiHeadAttention}","page":"Layers","title":"Base.show","text":"Base.show(io::IO, r::RotaryMultiHeadAttention)\n\nCustom string representation for RotaryMultiHeadAttention.\n\nArguments\n\nio::IO: The IO stream.\nr::RotaryMultiHeadAttention: An instance of RotaryMultiHeadAttention.\n\nExample\n\nr = RotaryMultiHeadAttention(256, 64, 8)\nprintln(r)\n\n\n\n\n\n","category":"method"},{"location":"layers/#VDJDL.Layers.Rezero","page":"Layers","title":"VDJDL.Layers.Rezero","text":"Rezero layer implementation for Flux.jl. This layer multiplies the input by a learnable parameter alpha. Note this layer leads to higher loss value as comapred to LayerNorm given everything else constant in our experiments, but it is faster.\n\nFields\n\nalpha::AbstractVector: The learnable parameter initialized by the user.\n\nConstructor\n\nRezero(init::AbstractVector): Create a new Rezero layer with the specified initial values for alpha.\nRezero(): Create a new Rezero layer with alpha initialized to zeros of type Float32 with length 1.\n\nMethods\n\n(a::Rezero)(x::AbstractArray): Apply the Rezero layer to the input array x.\n\n\n\n\n\n","category":"type"},{"location":"layers/#VDJDL.Layers.Rezero-Tuple{AbstractArray}","page":"Layers","title":"VDJDL.Layers.Rezero","text":"Apply the Rezero layer to the input array x.\n\nArguments\n\nx::AbstractArray: The input array.\n\nReturns\n\nAbstractArray: The transformed array after applying the Rezero layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#VDJDL.Layers.Rezero-Tuple{}","page":"Layers","title":"VDJDL.Layers.Rezero","text":"Create a new Rezero layer with alpha initialized to zeros of type Float32 with length 1.\n\nReturns\n\nRezero: A new instance of the Rezero layer.\n\n\n\n\n\n","category":"method"},{"location":"layers/#VDJDL.Layers.RotaryMultiHeadAttention","page":"Layers","title":"VDJDL.Layers.RotaryMultiHeadAttention","text":"struct RotaryMultiHeadAttention\n\nA struct representing a multi-head attention mechanism with rotary position embeddings.\n\nFields\n\nmha::MultiHeadAttention: An instance of MultiHeadAttention from Flux.\n\n\n\n\n\n","category":"type"},{"location":"layers/#VDJDL.Layers.RotaryMultiHeadAttention-Tuple{Any, Any, Any}","page":"Layers","title":"VDJDL.Layers.RotaryMultiHeadAttention","text":"(r::RotaryMultiHeadAttention)(q_in, k_in, v_in; bias=nothing, mask=nothing) -> Tuple\n\nApply the rotary multi-head attention mechanism to the input queries, keys, and values.\n\nArguments\n\nr::RotaryMultiHeadAttention: An instance of RotaryMultiHeadAttention.\nq_in: Input queries.\nk_in: Input keys.\nv_in: Input values.\nbias: Optional bias for the attention mechanism.\nmask: Optional mask for the attention mechanism.\n\nReturns\n\nA tuple (x, α) where x is the output of the attention mechanism and α is the attention weights.\n\n\n\n\n\n","category":"method"},{"location":"layers/#VDJDL.Layers.RotaryMultiHeadAttention-Tuple{Int64, Int64, Int64}","page":"Layers","title":"VDJDL.Layers.RotaryMultiHeadAttention","text":"RotaryMultiHeadAttention(hidden_size::Int, head_size::Int, nheads::Int; bias::Bool=false, dropout_prob::Float64=0.0) -> RotaryMultiHeadAttention\n\nCreate a RotaryMultiHeadAttention instance.\n\nArguments\n\nhidden_size::Int: The size of the hidden layer.\nhead_size::Int: The size of each attention head.\nnheads::Int: The number of attention heads.\nbias::Bool: Whether to use bias in the attention layers (default: false).\ndropout_prob::Float64: Dropout probability for the attention mechanism (default: 0.0).\n\nReturns\n\nA RotaryMultiHeadAttention instance.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = VDJDL","category":"page"},{"location":"#VDJ-Deep-Learning","page":"Home","title":"VDJ Deep Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"VDJDL (VDJ Deep Learning) is a Julia package specifically designed for deep learning tasks involving VDJ immunoreceptor sequences. It offers tools and functionalities for efficiently training deep learning models using the Flux library. VDJDL is tailored to support the unique challenges of working with VDJ recombination data, providing a robust framework for researchers and developers in computational biology and bioinformatics.","category":"page"},{"location":"#Modules","page":"Home","title":"Modules","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tokenizer\nEmebddings","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#Sequence-Tokenizer","page":"Home","title":"Sequence Tokenizer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Sequence Tokenizer provides a mapping between characters and integers, and vice versa.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using VDJDL: SequenceTokenizer\n\ntokenizer = SequenceTokenizer(collect(\"ATGC\"), '-')\ntokenizer(collect.([\"ATGC\", \"TGCTTG\", \"GGG\"]))","category":"page"},{"location":"#Positional-Encoding","page":"Home","title":"Positional Encoding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use sine-cosine positional embeddings, the following Flux-compatible layer is implemented.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using VDJDL: PositionEncoding\n\nemb_dim = 16\nseq_length = 100\npe = PositionEncoding(emb_dim)\npe(seq_length)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [VDJDL]","category":"page"},{"location":"tokenizer/","page":"Tokenizer","title":"Tokenizer","text":"CurrentModule = VDJDL.Tokenizer","category":"page"},{"location":"tokenizer/","page":"Tokenizer","title":"Tokenizer","text":"Modules = [Tokenizer]\nOrder   = [:function, :type]","category":"page"},{"location":"tokenizer/#VDJDL.Tokenizer.read_fasta-Tuple{String}","page":"Tokenizer","title":"VDJDL.Tokenizer.read_fasta","text":"read_fasta(filepath::String)\n\nReads a FASTA file and returns a vector of tuples containing the description and sequence of each record.\n\nArguments\n\nfilepath::String: The path to the FASTA file.\n\nReturns\n\nVector{Tuple{String, String}}: A vector of tuples with the description and sequence.\n\n\n\n\n\n","category":"method"},{"location":"tokenizer/#VDJDL.Tokenizer.LabelEncoder","page":"Tokenizer","title":"VDJDL.Tokenizer.LabelEncoder","text":"LabelEncoder(labels::Vector{String})\n\nEncodes and decodes labels.\n\nArguments\n\nlabels::Vector{String}: A vector of labels to be encoded.\n\nReturns\n\nLabelEncoder: A LabelEncoder object.\n\n\n\n\n\n","category":"type"},{"location":"tokenizer/#VDJDL.Tokenizer.SequenceTokenizer","page":"Tokenizer","title":"VDJDL.Tokenizer.SequenceTokenizer","text":"SequenceTokenizer(alphabet::Vector{T}, unksym::T) where T\n\nTokenizes character sequences.\n\nArguments\n\nalphabet::Vector{T}: A vector of symbols to be tokenized.\nunksym::T: The symbol for unknown tokens.\n\nReturns\n\nSequenceTokenizer{T}: A SequenceTokenizer object.\n\n\n\n\n\n","category":"type"}]
}
