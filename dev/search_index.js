var documenterSearchIndex = {"docs":
[{"location":"embeddings/","page":"Embeddings","title":"Embeddings","text":"CurrentModule = VDJDL.Embeddings","category":"page"},{"location":"embeddings/","page":"Embeddings","title":"Embeddings","text":"Modules = [Embeddings]\nOrder   = [:function, :type]","category":"page"},{"location":"embeddings/#Base.show-Tuple{IO, VDJDL.Embeddings.PositionEncoding}","page":"Embeddings","title":"Base.show","text":"Base.show(io::IO, pe::PositionEncoding)\n\nDisplays the PositionEncoding object.\n\nArguments\n\nio::IO: The I/O stream.\npe::PositionEncoding: The PositionEncoding object.\n\n\n\n\n\n","category":"method"},{"location":"embeddings/#VDJDL.Embeddings.make_position_encoding","page":"Embeddings","title":"VDJDL.Embeddings.make_position_encoding","text":"make_position_encoding(dim_embedding::Int, seq_length::Int, n::Int=10000)\n\nGenerates a positional encoding matrix.\n\nArguments\n\ndim_embedding::Int: The dimension of the embedding.\nseq_length::Int: The length of the sequence.\nn::Int: A scaling factor. Defaults to 10000.\n\nReturns\n\nMatrix{Float32}: A matrix containing positional encodings.\n\n\n\n\n\n","category":"function"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"PositionEncoding(dim_embedding::Int, max_length::Int=1000)\n\nConstructs a PositionEncoding object with specified embedding dimension and maximum sequence length.\n\nArguments\n\ndim_embedding::Int: The dimension of the embedding.\nmax_length::Int: The maximum length of the sequence. Defaults to 1000.\n\nReturns\n\nPositionEncoding: A PositionEncoding object.\n\n\n\n\n\n","category":"type"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-2","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"PositionEncoding{W <: AbstractArray}(weight::W)\n\nRepresents positional encodings for sequence data.\n\nArguments\n\nweight::W: The weight matrix for positional encodings.\n\n\n\n\n\n","category":"type"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-Tuple{AbstractArray}","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"(pe::PositionEncoding)(x::AbstractArray)\n\nApplies positional encoding to the input array.\n\nArguments\n\nx::AbstractArray: The input array.\n\nReturns\n\nThe positional encoding for the input array.\n\n\n\n\n\n","category":"method"},{"location":"embeddings/#VDJDL.Embeddings.PositionEncoding-Tuple{Int64}","page":"Embeddings","title":"VDJDL.Embeddings.PositionEncoding","text":"(pe::PositionEncoding)(seq_length::Int)\n\nRetrieves the positional encoding for a specific sequence length.\n\nArguments\n\nseq_length::Int: The length of the sequence.\n\nReturns\n\nA view of the positional encoding matrix for the given sequence length.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = VDJDL","category":"page"},{"location":"#VDJ-Deep-Learning","page":"Home","title":"VDJ Deep Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"VDJDL (VDJ Deep Learning) is a Julia package specifically designed for deep learning tasks involving VDJ immunoreceptor sequences. It offers tools and functionalities for efficiently training deep learning models using the Flux library. VDJDL is tailored to support the unique challenges of working with VDJ recombination data, providing a robust framework for researchers and developers in computational biology and bioinformatics.","category":"page"},{"location":"#Modules","page":"Home","title":"Modules","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tokenizer\nEmebddings","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#Sequence-Tokenizer","page":"Home","title":"Sequence Tokenizer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Sequence Tokenizer provides a mapping between characters and integers, and vice versa.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using VDJDL: SequenceTokenizer\n\ntokenizer = SequenceTokenizer(collect(\"ATGC\"), '-')\ntokenizer(collect.([\"ATGC\", \"TGCTTG\", \"GGG\"]))","category":"page"},{"location":"#Positional-Encoding","page":"Home","title":"Positional Encoding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use sine-cosine positional embeddings, the following Flux-compatible layer is implemented.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using VDJDL: PositionEncoding\n\nemb_dim = 16\nseq_length = 100\npe = PositionEncoding(emb_dim)\npe(seq_length)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [VDJDL]","category":"page"},{"location":"tokenizer/","page":"Tokenizer","title":"Tokenizer","text":"CurrentModule = VDJDL.Tokenizer","category":"page"},{"location":"tokenizer/","page":"Tokenizer","title":"Tokenizer","text":"Modules = [Tokenizer]\nOrder   = [:function, :type]","category":"page"},{"location":"tokenizer/#VDJDL.Tokenizer.read_fasta-Tuple{String}","page":"Tokenizer","title":"VDJDL.Tokenizer.read_fasta","text":"read_fasta(filepath::String)\n\nReads a FASTA file and returns a vector of tuples containing the description and sequence of each record.\n\nArguments\n\nfilepath::String: The path to the FASTA file.\n\nReturns\n\nVector{Tuple{String, String}}: A vector of tuples with the description and sequence.\n\n\n\n\n\n","category":"method"},{"location":"tokenizer/#VDJDL.Tokenizer.LabelEncoder","page":"Tokenizer","title":"VDJDL.Tokenizer.LabelEncoder","text":"LabelEncoder(labels::Vector{String})\n\nEncodes and decodes labels.\n\nArguments\n\nlabels::Vector{String}: A vector of labels to be encoded.\n\nReturns\n\nLabelEncoder: A LabelEncoder object.\n\n\n\n\n\n","category":"type"},{"location":"tokenizer/#VDJDL.Tokenizer.SequenceTokenizer","page":"Tokenizer","title":"VDJDL.Tokenizer.SequenceTokenizer","text":"SequenceTokenizer(alphabet::Vector{T}, unksym::T) where T\n\nTokenizes character sequences.\n\nArguments\n\nalphabet::Vector{T}: A vector of symbols to be tokenized.\nunksym::T: The symbol for unknown tokens.\n\nReturns\n\nSequenceTokenizer{T}: A SequenceTokenizer object.\n\n\n\n\n\n","category":"type"}]
}
